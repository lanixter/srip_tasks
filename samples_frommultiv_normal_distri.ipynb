{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "samples_frommultiv_normal_distri.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9E8csrxmqvt0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling from a multivariate normal distribution\n",
        "---"
      ],
      "metadata": {
        "id": "SUAwjRX0YOmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look to implement a sampling method to gather samples from a multivariate normal distribution, using random samples from a uniform distribution, and we use JAX library to implement the sampling the method. We have been successful in extracting samples from a multivariate normal distribution to a `p-value` of as low as `0.0006`.<br>\n",
        "The sampling method is described in detail below."
      ],
      "metadata": {
        "id": "J21_mtX0YcTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Parameters for the multivariate normal distribution**\n",
        "We have the following parameters to generate samples :\n",
        "1. Dimesion of the vector sampled from a multivariate mormal distribution, $d$ \n",
        "2. Number of sample vectors of $d$ dimesions, drawn from a multivariate normal distribution, taken as $N$\n",
        "3. Mean vector for the distributed data, $\\mu$\n",
        "4. Covariance Matrix, $\\Sigma$<br><br>\n",
        "Any multivariate normal distibution will random vectors of different dimensions, that is why it is neccessary to mention that the any vector $X$ drawn from a multivariate normal distibution will have a certain dimension $d$\n",
        "$$\n",
        "X \\sim N_d(\\mu,\\Sigma) = (x_1,x_2,...,x_d)^{T}\n",
        "$$<br> \n",
        "A larger sample of data $N$ captures more finer points in a distribution and tends to be more accurate, but is similarly associated with a significant time and more easily exposes the minute flaw in a sampling method.<p>\n",
        "The mean vector,  $μ$  is essentially the center of the distribution that result as the random samples from bivariate normal distribution. It's components  $μ_x$  and  $μ_y$  decribe the position of that center.<p>\n",
        "A covariance matrix  $Σ$ , by definition, is symmetric and positive definite, which means  $a^⊤Σa>0$  for all  $a∈R^d$ . A necessary and sufficient condition for  $Σ$  to be positive definite is that all of its eigenvalues are positive.<p>"
      ],
      "metadata": {
        "id": "xUYO6sXXZLLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Our Sampling Method**\n",
        "We have successfully developed a sampling method `mvn_samples()`, whereby we try to undertake the following steps : \n",
        "1. We first use `jax.numpy.linalgs.cholesky()` on our covariance matrix $K$ to get a Cholesky decomposition matrix $L$.\n",
        "2. We then use `jax.random.uniform()` to first generate two numbers from a uniform distribution and then we use the Box Muller Transform to map them to two numbers in a normal distribution.\n",
        "3. We generate a sample matrix $z$ with iteration on Step 2, which then undergoes a dot product, by using `np.dot()`, with our Cholesky decomposition matrix $L$ and added with our mean vector $\\mu$, we get a matrix $x$ of order $d \\times N$ which contains $N$ samples from a multivariate normal distribution of dimension $d$."
      ],
      "metadata": {
        "id": "m7Tq_NSihP7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Box Muller Transform**\n",
        "The Box Muller Transform allows us to sample from a pair of normally distributed variables using a source of only uniformly distributed variables. The transform is actually pretty simple to compute. We first sample two uniformly distributed random variables $U_1$ and $U_2$ using `jax.random.uniform()`. These two can then be mapped to two normally distributed variables $X$ and $Y$ as follows:\n",
        "$$\n",
        "X = R cos{Θ}\n",
        "\\\\Y = R sin{Θ}\n",
        "$$\n",
        "where $R = \\sqrt{2ln(U_1)}$ and $Θ = 2\\pi U_2$. This hence helps us with sampling for even dimensions. But for odd dimensions, an extra step of discarding an edge value of the sample is enough to help us generalise this algorithm for any dimension $d$."
      ],
      "metadata": {
        "id": "c9repdMCkGpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Error Calculations on the sampled data**\n",
        "We use `scipy.stats.kstest` to run a Kolmogorov-Smirnov Test on our sampled data. We, by using this test, pit our sampling method against a random sampling method over a multivariate normally distributed data. This test returns a `p-value` which is significant in our hypothesis testing.<p>\n",
        "**Interpreting P-Value** : A lower p-value is associated with a low error to occur when null hypothesis is rejected. In its very basic sense, it is a quantitative measure of : *How likely is the data, given that the null hypothesis is true?*<br>\n",
        "The null hypothesis for our experiment is : \"The sampled data is insignificant\"\n",
        "We have gathered an enough low p-value so as to successfully reject the null hypothesis."
      ],
      "metadata": {
        "id": "0diH03n0nSE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Code** "
      ],
      "metadata": {
        "id": "9E8csrxmqvt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M5khA-MHp5YD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dimension\n",
        "d = 10\n",
        "\n",
        "# no. of samples\n",
        "N = 10000\n",
        "\n",
        "# mean vector\n",
        "mu = jnp.array([0]*(d*N)).reshape(d,N)\n",
        "\n",
        "# set cov matrix (transform)\n",
        "K = jnp.identity(d)"
      ],
      "metadata": {
        "id": "8fNv2cps51im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744040bc-cfc4-4f55-8cae-4f4f98effe61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cholesky Decomposition on K\n",
        "L = jnp.linalg.cholesky(K)"
      ],
      "metadata": {
        "id": "1igw-gsq_bPb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mvn_samples(dim,num_samples):\n",
        "    i = 1\n",
        "    j = 1\n",
        "\n",
        "    z_o = []\n",
        "    z = []\n",
        "\n",
        "    while i <= num_samples:\n",
        "        while j <= dim:\n",
        "            key1 = jax.random.PRNGKey(random.randint(1, 1000))\n",
        "            key2 = jax.random.PRNGKey(random.randint(1, 1000))\n",
        "\n",
        "            u1 = jax.random.uniform(key1)\n",
        "            u2 = jax.random.uniform(key2)\n",
        "\n",
        "            n1 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)\n",
        "            n2 = math.sqrt(-2 * math.log(u1)) * math.sin(2 * math.pi * u2)\n",
        "\n",
        "            z_o.append(n1)\n",
        "            z_o.append(n2)\n",
        "            j += 2\n",
        "        if d%2 == 1: z_o.pop()\n",
        "        j = 1\n",
        "        i += 1\n",
        "    z = np.array(z_o).reshape(d,N)\n",
        "    return z"
      ],
      "metadata": {
        "id": "Fc5zLC5pTK1P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using our sampling method to generate N samples of d dimensions\n",
        "z = mvn_samples(d,N)"
      ],
      "metadata": {
        "id": "0xQxPbYgXscN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = mu + np.dot(L, z)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZGL8Go3FOuI",
        "outputId": "f62309f5-ebb5-4b66-e59a-f6ddcef40edc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-0.4937135 ,  0.62204444,  0.8375828 , ...,  1.398562  ,\n",
              "               0.5243517 ,  0.78688836],\n",
              "             [ 0.8740825 ,  0.20310648, -0.6787484 , ...,  0.9197775 ,\n",
              "              -0.36174822,  1.4511871 ],\n",
              "             [ 0.7331297 ,  0.9659406 , -0.6709099 , ...,  0.5670589 ,\n",
              "               0.79626155,  0.69782096],\n",
              "             ...,\n",
              "             [-0.41330576, -0.12295498, -1.1500207 , ...,  0.32302696,\n",
              "               0.06485996,  1.1162292 ],\n",
              "             [-0.9599803 ,  1.097428  ,  0.1392937 , ...,  0.13469744,\n",
              "              -0.43227312, -1.1445631 ],\n",
              "             [-0.653419  , -0.48580903, -0.8741085 , ...,  0.4594269 ,\n",
              "              -2.1363518 ,  1.0201639 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error Calculations on the sampled data\n",
        "for i in range(d):\n",
        "    test_stat, pvalue = stats.kstest(x[i], 'norm', args=(0, 1), N=N*d)\n",
        "    print(\"sample_N[%d](mu,K) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (i,test_stat, pvalue))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXLdVzJWYeb5",
        "outputId": "6f16a603-7c41-4d12-8317-0caefe0f011d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_N[0](mu,K) vs. N(0, 1): KS=0.0107 with p-value = 0.1999.\n",
            "sample_N[1](mu,K) vs. N(0, 1): KS=0.0120 with p-value = 0.1120.\n",
            "sample_N[2](mu,K) vs. N(0, 1): KS=0.0097 with p-value = 0.3043.\n",
            "sample_N[3](mu,K) vs. N(0, 1): KS=0.0190 with p-value = 0.0015.\n",
            "sample_N[4](mu,K) vs. N(0, 1): KS=0.0184 with p-value = 0.0024.\n",
            "sample_N[5](mu,K) vs. N(0, 1): KS=0.0135 with p-value = 0.0516.\n",
            "sample_N[6](mu,K) vs. N(0, 1): KS=0.0166 with p-value = 0.0082.\n",
            "sample_N[7](mu,K) vs. N(0, 1): KS=0.0133 with p-value = 0.0583.\n",
            "sample_N[8](mu,K) vs. N(0, 1): KS=0.0106 with p-value = 0.2095.\n",
            "sample_N[9](mu,K) vs. N(0, 1): KS=0.0201 with p-value = 0.0006.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **References**\n",
        "1. [Multivariate Normal Distribution](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjWysnjgor3AhUhCqYKHRypCRIQFnoECB0QAQ&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMultivariate_normal_distribution&usg=AOvVaw3C61xV1Xhz133Z12_TArIW)\n",
        "\n",
        "2. [Generation of Normally Distributed Numbers](https://www.projectrhea.org/rhea/index.php/Generation_of_N-dimensional_normally_distributed_random_numbers_from_two_categories_with_different_priors)\n",
        "\n",
        "3. [Box Muller Transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)\n",
        "\n",
        "4. [Cholesky Decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition)"
      ],
      "metadata": {
        "id": "nvU8tEsupaC4"
      }
    }
  ]
}